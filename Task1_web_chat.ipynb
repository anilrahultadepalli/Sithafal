{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEi8Xf1kcRPU"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.uchicago.edu/\""
      ],
      "metadata": {
        "id": "Q4Jp5ijhcV7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def c_headers():\n",
        "    return {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"\n",
        "    }\n",
        "def fpc(page_url, request_headers):\n",
        "    page_response = requests.get(page_url, headers=request_headers)\n",
        "    return page_response\n",
        "def extract_content(page_response):\n",
        "    page_soup = BeautifulSoup(page_response.text, \"html.parser\")\n",
        "    content_structure = {\n",
        "        \"page_url\": page_response.url,\n",
        "        \"headings\": [],\n",
        "        \"paragraphs\": [],\n",
        "        \"miscellaneous_text\": []\n",
        "    }\n",
        "    for hl in range(1, 7):\n",
        "        for hg in page_soup.find_all(f\"h{hl}\"):\n",
        "            heading_text = hg.get_text(strip=True)\n",
        "            if heading_text:\n",
        "                content_structure[\"headings\"].append(heading_text)\n",
        "    for pg in page_soup.find_all(\"p\"):\n",
        "        paragraph_text = pg.get_text(strip=True)\n",
        "        if paragraph_text:\n",
        "            content_structure[\"paragraphs\"].append(paragraph_text)\n",
        "    for at in page_soup.find_all(True):\n",
        "        if at.name not in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\"]:\n",
        "            tag_text = at.get_text(strip=True)\n",
        "            if tag_text:\n",
        "                content_structure[\"miscellaneous_text\"].append(tag_text)\n",
        "    content_structure[\"headings\"] = list(dict.fromkeys(content_structure[\"headings\"]))\n",
        "    content_structure[\"paragraphs\"] = list(dict.fromkeys(content_structure[\"paragraphs\"]))\n",
        "    content_structure[\"miscellaneous_text\"] = list(dict.fromkeys(content_structure[\"miscellaneous_text\"]))\n",
        "    return content_structure\n",
        "def spd(page_url):\n",
        "    headers = c_headers()\n",
        "    response = fpc(page_url, headers)\n",
        "    if response.status_code != 200:\n",
        "        return {\n",
        "            \"page_url\": page_url,\n",
        "            \"error\": f\"Failed to retrieve content, status code: {response.status_code}\"\n",
        "        }\n",
        "    return extract_content(response)\n"
      ],
      "metadata": {
        "id": "H7Itnw89cXer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sdtf(scraped_data, output_directory):\n",
        "    domain_name = scraped_data[\"url\"].split(\"//\")[-1].split(\"/\")[0]\n",
        "    filename = f\"{domain_name}.json\"\n",
        "    file_location = os.path.join(output_directory, filename)\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "    with open(file_location, \"w\", encoding=\"utf-8\") as file:\n",
        "        json.dump(scraped_data, file, indent=4, ensure_ascii=False)\n",
        "    print(f\"Saved data to {file_location}\")"
      ],
      "metadata": {
        "id": "AHbrs7RPcaqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2jW0ne5_ceKS",
        "outputId": "0dd7cd70-2876-4927-b647-6c6ae076c699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoTY22TYcgJF",
        "outputId": "bd283d49-ca43-416f-fc3d-97f9d2c7d6c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "def retrieve_data(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return {\"error\": \"Failed to retrieve data\"}"
      ],
      "metadata": {
        "id": "IAhEhjCVcldQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}